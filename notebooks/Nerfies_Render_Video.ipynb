{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nerfies Render Video.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMWf9AQcdlp"
      },
      "source": [
        "# Render a Nerfie video!\n",
        "\n",
        "**Author**: [Keunhong Park](https://keunhong.com)\n",
        "\n",
        "[[Project Page](https://nerfies.github.io)]\n",
        "[[Paper](https://storage.googleapis.com/nerfies-public/videos/nerfies_paper.pdf)]\n",
        "[[Video](https://www.youtube.com/watch?v=MrKrnHhk8IA)]\n",
        "[[GitHub](https://github.com/google/nerfies)]\n",
        "\n",
        "This notebook renders a figure-8 orbit video using the test cameras generated in the capture processing notebook.\n",
        "\n",
        "You can also load your own custom cameras by modifying the code slightly.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Convert a video into our dataset format using the [capture processing notebook](https://colab.sandbox.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Capture_Processing.ipynb).\n",
        "2. Train a Nerfie model using the [training notebook](https://colab.sandbox.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Training.ipynb)\n",
        "3. Run this notebook!\n",
        "\n",
        "\n",
        "### Notes\n",
        " * Please report issues on the [GitHub issue tracker](https://github.com/google/nerfies/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHqkIo4hcGou"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GwSf5FfcH4b"
      },
      "source": [
        "!pip install flax frozendict ipyplot\n",
        "!pip install git+https://github.com/google/nerfies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-3T2lBKBcIGP"
      },
      "source": [
        "# @title Configure notebook runtime\n",
        "# @markdown If you would like to use a GPU runtime instead, change the runtime type by going to `Runtime > Change runtime type`. \n",
        "# @markdown You will have to use a smaller batch size on GPU.\n",
        "\n",
        "runtime_type = 'tpu'  # @param ['gpu', 'tpu']\n",
        "if runtime_type == 'tpu':\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "print('Detected Devices:', jax.devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82kU-W1NcNTW",
        "cellView": "form"
      },
      "source": [
        "# @title Mount Google Drive\n",
        "# @markdown Mount Google Drive onto `/content/gdrive`. You can skip this if running locally.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIDbV769cPn1",
        "cellView": "form"
      },
      "source": [
        "# @title Define imports and utility functions.\n",
        "\n",
        "import jax\n",
        "from jax.config import config as jax_config\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax import jax_utils\n",
        "from flax import optim\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import checkpoints\n",
        "jax_config.enable_omnistaging() # Linen requires enabling omnistaging\n",
        "\n",
        "from absl import logging\n",
        "from io import BytesIO\n",
        "import random as pyrandom\n",
        "import numpy as np\n",
        "import PIL\n",
        "import IPython\n",
        "import tempfile\n",
        "import imageio\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "# Monkey patch logging.\n",
        "def myprint(msg, *args, **kwargs):\n",
        " print(msg % args)\n",
        "\n",
        "logging.info = myprint \n",
        "logging.warn = myprint\n",
        "logging.error = myprint\n",
        "\n",
        "\n",
        "def show_image(image, fmt='png'):\n",
        "    image = image_utils.image_to_uint8(image)\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(image).save(f, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "\n",
        "\n",
        "def show_video(frames, fps=30):\n",
        "  with tempfile.NamedTemporaryFile(suffix='.mp4') as f:\n",
        "    with imageio.get_writer(f.name, fps=fps) as writer:\n",
        "      for frame in frames:\n",
        "        writer.append_data(frame)\n",
        "\n",
        "    with open(f.name,'rb') as f:\n",
        "      data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2QYJ7dyMcw2f"
      },
      "source": [
        "# @title Model and dataset configuration\n",
        "# @markdown Change the directories to where you saved your capture and experiment.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import gin\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from nerfies import configs\n",
        "\n",
        "\n",
        "# @markdown The working directory where the trained model is.\n",
        "train_dir = '/content/gdrive/My Drive/nerfies/experiments/capture1/exp1'  # @param {type: \"string\"}\n",
        "# @markdown The directory to the dataset capture.\n",
        "data_dir = '/content/gdrive/My Drive/nerfies/captures/capture1'  # @param {type: \"string\"}\n",
        "\n",
        "checkpoint_dir = Path(train_dir, 'checkpoints')\n",
        "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "config_path = Path(train_dir, 'config.gin')\n",
        "with open(config_path, 'r') as f:\n",
        "  logging.info('Loading config from %s', config_path)\n",
        "  config_str = f.read()\n",
        "gin.parse_config(config_str)\n",
        "\n",
        "config_path = Path(train_dir, 'config.gin')\n",
        "with open(config_path, 'w') as f:\n",
        "  logging.info('Saving config to %s', config_path)\n",
        "  f.write(config_str)\n",
        "\n",
        "exp_config = configs.ExperimentConfig()\n",
        "model_config = configs.ModelConfig()\n",
        "train_config = configs.TrainConfig()\n",
        "eval_config = configs.EvalConfig()\n",
        "\n",
        "display(Markdown(\n",
        "    gin.config.markdownify_operative_config_str(gin.operative_config_str())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6T7LQ5QSmu4o"
      },
      "source": [
        "# @title Create datasource and show an example.\n",
        "\n",
        "from nerfies import datasets\n",
        "from nerfies import image_utils\n",
        "\n",
        "datasource = datasets.from_config(\n",
        "  exp_config.datasource_spec,\n",
        "  image_scale=exp_config.image_scale,\n",
        "  use_appearance_id=model_config.use_appearance_metadata,\n",
        "  use_camera_id=model_config.use_camera_metadata,\n",
        "  use_warp_id=model_config.use_warp,\n",
        "  random_seed=exp_config.random_seed)\n",
        "\n",
        "show_image(datasource.load_rgb(datasource.train_ids[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEO3xcxpnCqx",
        "cellView": "form"
      },
      "source": [
        "# @title Initialize model\n",
        "# @markdown Defines the model and initializes its parameters.\n",
        "\n",
        "from flax.training import checkpoints\n",
        "from nerfies import models\n",
        "from nerfies import model_utils\n",
        "from nerfies import schedules\n",
        "from nerfies import training\n",
        "\n",
        "\n",
        "rng = random.PRNGKey(exp_config.random_seed)\n",
        "np.random.seed(exp_config.random_seed + jax.host_id())\n",
        "devices = jax.devices()\n",
        "\n",
        "learning_rate_sched = schedules.from_config(train_config.lr_schedule)\n",
        "warp_alpha_sched = schedules.from_config(train_config.warp_alpha_schedule)\n",
        "elastic_loss_weight_sched = schedules.from_config(\n",
        "    train_config.elastic_loss_weight_schedule)\n",
        "\n",
        "rng, key = random.split(rng)\n",
        "params = {}\n",
        "model, params['model'] = models.nerf(\n",
        "    key,\n",
        "    model_config,\n",
        "    batch_size=train_config.batch_size,\n",
        "    num_appearance_embeddings=len(datasource.appearance_ids),\n",
        "    num_camera_embeddings=len(datasource.camera_ids),\n",
        "    num_warp_embeddings=len(datasource.warp_ids),\n",
        "    near=datasource.near,\n",
        "    far=datasource.far,\n",
        "    use_warp_jacobian=train_config.use_elastic_loss,\n",
        "    use_weights=train_config.use_elastic_loss)\n",
        "\n",
        "optimizer_def = optim.Adam(learning_rate_sched(0))\n",
        "optimizer = optimizer_def.create(params)\n",
        "state = model_utils.TrainState(\n",
        "    optimizer=optimizer,\n",
        "    warp_alpha=warp_alpha_sched(0))\n",
        "scalar_params = training.ScalarParams(\n",
        "    learning_rate=learning_rate_sched(0),\n",
        "    elastic_loss_weight=elastic_loss_weight_sched(0),\n",
        "    background_loss_weight=train_config.background_loss_weight)\n",
        "logging.info('Restoring checkpoint from %s', checkpoint_dir)\n",
        "state = checkpoints.restore_checkpoint(checkpoint_dir, state)\n",
        "step = state.optimizer.state.step + 1\n",
        "state = jax_utils.replicate(state, devices=devices)\n",
        "del params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KYhbpsklwAy",
        "cellView": "form"
      },
      "source": [
        "# @title Define pmapped render function.\n",
        "\n",
        "import functools\n",
        "from nerfies import evaluation\n",
        "\n",
        "devices = jax.devices()\n",
        "\n",
        "\n",
        "def _model_fn(key_0, key_1, params, rays_dict, alpha):\n",
        "  out = model.apply({'params': params},\n",
        "                    rays_dict,\n",
        "                    warp_alpha=alpha,\n",
        "                    rngs={\n",
        "                        'coarse': key_0,\n",
        "                        'fine': key_1\n",
        "                    },\n",
        "                    mutable=False)\n",
        "  return jax.lax.all_gather(out, axis_name='batch')\n",
        "\n",
        "pmodel_fn = jax.pmap(\n",
        "    # Note rng_keys are useless in eval mode since there's no randomness.\n",
        "    _model_fn,\n",
        "    # key0, key1, params, rays_dict, alpha\n",
        "    in_axes=(0, 0, 0, 0, 0),\n",
        "    devices=devices,\n",
        "    donate_argnums=(3,),  # Donate the 'rays' argument.\n",
        "    axis_name='batch',\n",
        ")\n",
        "\n",
        "render_fn = functools.partial(evaluation.render_image,\n",
        "                              model_fn=pmodel_fn,\n",
        "                              device_count=len(devices),\n",
        "                              chunk=eval_config.chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Fq0kNcmAra"
      },
      "source": [
        "# @title Load cameras.\n",
        "\n",
        "from nerfies import utils\n",
        "\n",
        "\n",
        "test_camera_paths = datasource.glob_cameras(Path(data_dir, 'camera-paths/orbit-mild'))\n",
        "test_cameras = utils.parallel_map(datasource.load_camera, test_camera_paths, show_pbar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP9LjiAZmoRc",
        "cellView": "form"
      },
      "source": [
        "# @title Render video frames.\n",
        "import ipyplot\n",
        "from nerfies import visualization as viz\n",
        "\n",
        "\n",
        "rng = rng + jax.host_id()  # Make random seed separate across hosts.\n",
        "keys = random.split(rng, len(devices))\n",
        "\n",
        "results = []\n",
        "for i in range(len(test_cameras)):\n",
        "  print(f'Rendering frame {i+1}/{len(test_cameras)}')\n",
        "  camera = test_cameras[i]\n",
        "  batch = datasets.camera_to_rays(camera)\n",
        "  batch['metadata'] = {\n",
        "      'appearance': jnp.zeros_like(batch['origins'][..., 0, jnp.newaxis], jnp.uint32),\n",
        "      'warp': jnp.zeros_like(batch['origins'][..., 0, jnp.newaxis], jnp.uint32),\n",
        "  }\n",
        "\n",
        "  pred_color, pred_depth, pred_depth_med, pred_acc = render_fn(state, batch, rng=rng)\n",
        "  results.append((pred_color, pred_depth))\n",
        "  pred_depth_viz = viz.colorize(pred_depth.squeeze(), cmin=datasource.near, cmax=datasource.far, invert=True)\n",
        "  show_image(np.array(pred_color))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_5hHR9XVm8Ix"
      },
      "source": [
        "# @title Show rendered video.\n",
        "\n",
        "fps = 30  # @param {type:'number'}\n",
        "\n",
        "frames = []\n",
        "for rgb, depth in results:\n",
        "  depth_viz = viz.colorize(depth.squeeze(), cmin=datasource.near, cmax=datasource.far, invert=True)\n",
        "  frame = np.concatenate([rgb, depth_viz], axis=1)\n",
        "  frames.append(image_utils.image_to_uint8(frame))\n",
        "\n",
        "show_video(frames, fps=fps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}